<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Units 5–7 MCQs</title>
<style>
  body{font-family:Arial,Helvetica,sans-serif;margin:0;padding:1.5rem;background:#f9fafb;color:#111}
  h1{text-align:center;font-size:1.9rem;margin:0 0 1rem;font-weight:700}
  .note{background:#e2e8f0;color:#334155;border-radius:6px;padding:0.75rem;margin-bottom:1.5rem;font-size:0.9rem}
  .question{background:#fff;border:1px solid #d1d5db;border-radius:8px;margin-bottom:1.25rem;padding:1rem}
  .question p{margin:0 0 0.75rem;font-weight:600}
  .options label{display:block;margin:0.3rem 0;cursor:pointer}
  .feedback{margin-top:0.6rem;font-weight:600}
  .correct{color:#16a34a}
  .incorrect{color:#dc2626}
</style>
</head>
<body>
<h1>Units 5–7 MCQs</h1>
<div class="note">Note: The questions were generated by AI</div>
<div id="quiz"></div>
<script>
const quizData=[
// -------------- Unit 5 – Interpolation (15) --------------
{stem:`Which fact guarantees that the Newton divided‑difference interpolating polynomial of degree n through n + 1 distinct abscissae is <em>unique</em>?`,opts:[`Existence of Chebyshev nodes`,`Existence of a non‑singular Lagrange basis through the data points`,`Symmetry of divided differences`,`Periodicity of the data`],ans:1,exp:`With n + 1 distinct x‑values the Lagrange basis polynomials are linearly independent, giving a nonsingular system. Hence only one polynomial satisfies the interpolation conditions.`},
{stem:`For an n‑degree Lagrange polynomial P<sub>n</sub>(x) approximating f(x), the leading truncation error involves`,opts:[`f<sup>(n)</sup>(ξ) and h<sup>n</sup>`,`f<sup>(n+1)</sup>(ξ) multiplied by ∏(x – x<sub>i</sub>)`,`f<sup>(n‑1)</sup>(ξ) only`,`No derivative term at all`],ans:1,exp:`The generalized remainder term is f<sup>(n+1)</sup>(ξ)/(n+1)! · ∏(x – x<sub>i</sub>).`},
{stem:`Compared with a single high‑degree global polynomial, a <em>cubic spline</em> fitted to the same data usually`,opts:[`gives a lower maximum interpolation error for smooth data`,`requires no solution of linear systems`,`automatically preserves monotonicity in every interval`,`has a larger Runge‑type oscillation near the ends`],ans:0,exp:`Piece‑wise cubics keep each polynomial degree low, reducing oscillations and the worst‑case error.`},
{stem:`The k‑th forward (or divided) difference of any polynomial of degree k – 1 is`,opts:[`a non‑zero constant`,`exactly zero`,`equal to the first derivative of the polynomial`,`undefined unless the nodes are equally spaced`],ans:1,exp:`Successive differences act like discrete derivatives; once the order exceeds the degree, the difference vanishes.`},
{stem:`Newton’s <em>forward</em> interpolation formula assumes that the tabulated x‑values are`,opts:[`equally spaced`,`clustered at Chebyshev nodes`,`monotone but not necessarily uniform`,`symmetrically placed about x = 0`],ans:0,exp:`Forward (and backward) difference tables rely on a constant spacing h to factor out powers of h in the coefficients.`},
{stem:`A <em>natural</em> cubic spline satisfies which boundary condition at the endpoints?`,opts:[`y′ = 0`,`y″ = 0`,`y = 0`,`y‴ = 0`],ans:1,exp:`Setting the second derivative to zero at both ends avoids artificial curvature and fully determines the spline.`},
{stem:`Evaluating a Lagrange interpolant with the <em>barycentric</em> formula, instead of the naïve formula, lowers the computational cost from`,opts:[`O(n³) to O(n²)`,`O(n²) to O(n)`,`O(n) to O(log n)`,`O(n²) to O(1)`],ans:1,exp:`The barycentric weights can be pre‑computed; evaluation then becomes a single O(n) weighted sum.`},
{stem:`The forward‑difference table for n + 1 equally spaced data points collapses to an all‑zero k‑th row <em>exactly when</em> the original data come from a polynomial of degree`,opts:[`k – 1`,`k`,`k + 1`,`2k – 1`],ans:0,exp:`A degree‑(k – 1) polynomial’s k‑th discrete derivative (difference) is identically zero, mirroring the continuous case.`},
{stem:`<em>Inverse interpolation</em> is normally chosen when`,opts:[`x = f<sup>‑1</sup>(y) is monotone and a root near the data range is sought`,`the data are contaminated by noise`,`the polynomial degree exceeds 10`,`endpoint derivatives are unknown`],ans:0,exp:`Solving for x as a function of y and then setting y = 0 yields a root without bracketing if x(y) is monotone.`},
{stem:`Differentiating an n‑degree interpolating polynomial gives a derivative estimate whose truncation error at a tabulated node is O`,opts:[`(h)`,`(h²)`,`(h<sup>n</sup>)`,`(h<sup>n‑1</sup>)`],ans:3,exp:`One power of h is lost when differentiating the error term, so the order drops by one.`},
{stem:`When two abscissae coincide in divided‑difference interpolation, the repeated denominator is replaced by`,opts:[`zero`,`the corresponding derivative value (Hermite data)`,`the arithmetic mean of neighbouring abscissae`,`a spline knot value`],ans:1,exp:`The divided‑difference formula tends to a derivative limit; Hermite interpolation uses those derivative data explicitly.`},
{stem:`Global high‑degree polynomial interpolation on equispaced nodes is prone to large oscillations near the ends, a behaviour mitigated by`,opts:[`switching to piece‑wise lower‑degree polynomials (splines)`,`increasing the polynomial degree further`,`halving the step size without changing degree`,`using rounded coefficients instead of exact fractions`],ans:0,exp:`Splines keep each individual polynomial degree low, preventing the large endpoint wiggles.`},
{stem:`The main arithmetic cost in constructing Newton’s divided‑difference table for n + 1 points is`,opts:[`O(n²) subtractions`,`O(n) multiplications`,`O(n³) additions`,`O(2ⁿ) evaluations`],ans:0,exp:`There are (n(n+1))/2 off‑diagonal differences to compute, giving an O(n²) effort.`},
{stem:`The <em>error constant</em> for linear interpolation (two nodes) involves which derivative?`,opts:[`f′(ξ)`,`f″(ξ)`,`f‴(ξ)`,`no derivative; linear interpolation is exact`],ans:1,exp:`The remainder of the degree‑1 Taylor approximation contains the second derivative term.`},
{stem:`Using more data points than necessary to fit a low‑degree interpolant is typically handled by`,opts:[`least‑squares regression`,`divided differences`,`Hermite replacement`,`inverse interpolation`],ans:0,exp:`With > n + 1 points, exact interpolation is impossible; a best‑fit (least‑squares) polynomial is used instead.`},
// -------------- Unit 6 – Numerical Differentiation (15) --------------
{stem:`The forward difference [f(x+h) – f(x)]/h approximates f′(x) with a leading truncation error of order`,opts:[`O(h)`,`O(h²)`,`O(h³)`,`O(h‑1)`],ans:0,exp:`Expanding f(x+h) in a Taylor series shows the first neglected term is (h/2)f″(ξ).`},
{stem:`A three‑point <em>centered</em> difference for f′(x) has a truncation error proportional to`,opts:[`h`,`h²`,`h³`,`h⁴`],ans:1,exp:`Symmetry cancels the odd‑power terms, giving an O(h²) error.`},
{stem:`A five‑point centered stencil can raise the accuracy for f′(x) to`,opts:[`O(h²)`,`O(h³)`,`O(h⁴)`,`O(h⁵)`],ans:2,exp:`Including two more symmetric points cancels the next two Taylor terms, leaving the h⁴ term dominant.`},
{stem:`Balancing truncation and round‑off errors for a three‑point centered difference yields an <em>optimal</em> step size h proportional to`,opts:[`√(ε)`,`√(ε/M)` ,`ε/M`,`ε²/M`],ans:1,exp:`Setting the O(h²) truncation equal to the O(ε/h) round‑off term gives h≈√(ε/M).`},
{stem:`If h is reduced <em>beyond</em> the optimal value, the total error will`,opts:[`continue to decrease quadratically`,`flatten out`,`increase because round‑off dominates`,`remain constant`],ans:2,exp:`Round‑off grows like ε/h and eventually outweighs the shrinking truncation term.`},
{stem:`A five‑point <em>forward</em> difference for f′(x) requires how many function evaluations?`,opts:[`3`,`4`,`5`,`6`],ans:2,exp:`Points x, x+h, x+2h, x+3h, x+4h are used—five values in total.`},
{stem:`The classical formula [f(x+h) – 2f(x) + f(x‑h)]/h² approximates f″(x) with accuracy`,opts:[`O(h)`,`O(h²)`,`O(h³)`,`O(h⁴)`],ans:1,exp:`Its error term is (h²/12)f⁽⁴⁾(ξ).`},
{stem:`Finite‑difference formulas built from Taylor expansions are <em>exact</em> for polynomials of degree up to`,opts:[`the number of stencil points minus 1`,`one less than the truncation order`,`twice the number of points minus 1`,`any degree—no restriction`],ans:0,exp:`Because the Taylor expansion is exact for a polynomial of sufficiently low degree, the difference formula reproduces it exactly.`},
{stem:`Subtractive cancellation is worst when`,opts:[`h is large`,`f(x+h) ≈ f(x‑h)`,`the function is monotone`,`double precision is used`],ans:1,exp:`Nearly equal numbers lose significant digits during subtraction, magnifying round‑off.`},
{stem:`To obtain an O(h⁴) approximation for f′(x) <em>without</em> extra function calls, one normally`,opts:[`applies a higher‑order forward formula`,`averages forward and backward O(h) estimates`,`uses a wider centered stencil`,`this cannot be done without extra calls`],ans:2,exp:`A five‑point centered stencil reuses existing symmetric evaluations to cancel more terms.`},
{stem:`For an <em>odd</em> polynomial about the origin, the three‑point centered difference at x = 0 is`,opts:[`exact`,`off by O(h)`,`off by O(h²)`,`undefined because f is odd`],ans:0,exp:`All even‑power terms vanish; the difference reproduces the derivative exactly.`},
{stem:`When tabulated data contain an unknown noise level, a reasonable step size for finite differences is chosen by`,opts:[`minimizing the theoretical truncation error only`,`looking for the turning point on a log‑log error vs h plot`,`making h as small as the data spacing allows`,`pure trial‑and‑error with no theory`],ans:1,exp:`Plotting error estimates against h often shows a minimum where truncation and noise effects balance.`},
{stem:`The product h·f‴(ξ) appearing in the forward‑difference error term originates from`,opts:[`the first neglected Taylor term`,`round‑off analysis`,`divided differences`,`central averaging coefficients`],ans:0,exp:`It is simply the next term retained when the Taylor series is truncated after f″.`},
{stem:`Directional derivatives in multiple dimensions can be approximated by one‑dimensional finite differences along`,opts:[`coordinate axes only`,`any chosen vector`,`circles of radius h`,`level curves of f`],ans:1,exp:`Evaluating f(x+hv) and f(x) along any unit vector v gives the directional derivative v·∇f.`},
{stem:`Increasing the order of a finite‑difference formula generally <em>reduces</em> truncation error but may`,opts:[`increase sensitivity to round‑off and data noise`,`make the stencil symmetric`,`eliminate the need for step‑size tuning`,`violate the mean‑value theorem`],ans:0,exp:`More points and larger coefficients amplify round‑off and any measurement noise present in the data.`},
// -------------- Unit 7 – Numerical Integration (15) --------------
{stem:`The error of a single‑segment trapezoidal rule over [a,b] is E ≈`,opts:[`(b‑a)f′(ξ)`,`−(b‑a)h²f″(ξ)/12`,`−h³f″(ξ)/12`,`−h²f″(ξ)/12`],ans:3,exp:`Deriving from integrating the quadratic remainder of linear interpolation yields −h²f″(ξ)/12.`},
{stem:`For n equal segments, the <em>composite</em> trapezoidal rule has global error order`,opts:[`O(h)`,`O(h²)`,`O(h³)`,`O(h⁴)`],ans:1,exp:`Each panel error is O(h³); summing n ≈ (b‑a)/h panels gives an overall O(h²) error.`},
{stem:`Simpson’s 1/3 rule integrates <em>exactly</em> every polynomial of degree`,opts:[`1`,`2`,`3`,`4`],ans:2,exp:`It is a closed Newton–Cotes rule of degree 3.`},
{stem:`Simpson’s 3/8 rule requires the number of segments n to be`,opts:[`even`,`odd`,`a multiple of 3`,`a power of 2`],ans:2,exp:`The 3/8 pattern spans three panels; therefore n must be divisible by 3.`},
{stem:`When n = 5 segments are needed, a common recommendation is to`,opts:[`apply the 1/3 rule five times`,`combine four 1/3 panels with one 3/8 panel`,`fall back to the composite trapezoidal rule`,`switch to a two‑point Gaussian rule`],ans:1,exp:`Four panels satisfy the 1/3 restriction (even n) and the last three panels form a 3/8 group.`},
{stem:`In the composite trapezoidal rule, the coefficient applied to each <em>interior</em> ordinate is`,opts:[`1`,`2`,`3`,`4`],ans:1,exp:`The endpoints are weighted 1; all other ordinates are doubled because each interior segment shares two panels.`},
{stem:`Halving the step size in the composite trapezoidal rule typically reduces the error by a factor of roughly`,opts:[`2`,`4`,`8`,`16`],ans:1,exp:`Because the global error is O(h²), E(h/2) ≈ E(h)/4.`},
{stem:`Composite Simpson’s 1/3 rule can be applied only when the number of segments n is`,opts:[`odd`,`even`,`prime`,`a multiple of 3`],ans:1,exp:`Each parabolic panel spans two segments, so n must be even.`},
{stem:`Simpson’s 1/3 rule uses weights, in order,`,opts:[`1, 1`,`1, 2, 1`,`1, 4, 1`,`1, 3, 3, 1`],ans:2,exp:`The rule integrates a quadratic fit through three equally spaced points → weights 1‑4‑1.`},
{stem:`The leading error term of the composite Simpson’s 1/3 rule is E ≈`,opts:[`−(b‑a)h²f″(ξ)/12`,`−(b‑a)h⁴f⁽⁴⁾(ξ)/180`,`−h³f‴(ξ)/12`,`−h⁵f⁽⁵⁾(ξ)/90`],ans:1,exp:`Integrating the cubic remainder shows the first non‑zero term involves the fourth derivative.`},
{stem:`For smooth, 2π‑periodic functions sampled on a uniform grid, the composite trapezoidal rule converges`,opts:[`linearly in h`,`quadratically in h`,`super‑algebraically (error ∝ e^{‑const/h})`,`not at all`],ans:2,exp:`The periodic endpoint match cancels all even‑order terms, giving extremely fast convergence.`},
{stem:`Closed Newton–Cotes formulas of degree n are derived by`,opts:[`interpolating f by an n‑degree polynomial and integrating it`,`matching f to Chebyshev polynomials`,`differentiating midpoint rules`,`minimizing the squared error of the weights`],ans:0,exp:`Integrate the unique interpolating polynomial through equally spaced nodes to obtain the weights.`},
{stem:`The composite Simpson’s 3/8 rule integrates exactly any polynomial of degree up to`,opts:[`2`,`3`,`4`,`5`],ans:1,exp:`Both Simpson variants are third‑degree accurate.`},
{stem:`In Simpson’s 1/3 rule, the coefficients 4 and 2 alternate because`,opts:[`the underlying quadratic is even`,`odd ordinates appear in two parabolic panels, even ordinates in one`,`it enforces periodicity`,`they cancel truncation error terms`],ans:1,exp:`Interior odd‑index points are shared by two adjacent parabolas; even‑index points belong to one.`},
{stem:`The trapezoidal rule can be derived directly by integrating`,opts:[`a linear interpolant between consecutive nodes`,`a constant function equal to the average of endpoints`,`a quadratic interpolant`,`the first derivative of f`],ans:0,exp:`Connecting the two endpoints by a straight line gives the trapezoid whose area equals the integral of that line.`},
// -------------- Mixed review extras (5) --------------
{stem:`Round‑off <em>error</em> differs from truncation error in that round‑off arises from`,opts:[`using a finite number of significant digits`,`approximating an infinite series by a finite one`,`neglecting higher‑order spatial derivatives`,`choosing an h that is too large`],ans:0,exp:`Round‑off is the inevitable representation error of floating‑point arithmetic.`},
{stem:`Bracketing methods for root finding (e.g., bisection) require`,opts:[`two initial guesses with opposite signs of f`,`an initial derivative estimate`,`only one guess but twice the iterations`,`equally spaced guesses`],ans:0,exp:`Bracketing relies on the Intermediate Value Theorem, which needs f(a)·f(b)<0.`},
{stem:`Open methods such as Newton‑Raphson are generally <em>faster</em> than bracketing methods because`,opts:[`they use tangent information to converge quadratically`,`they always converge regardless of the initial guess`,`they avoid evaluating derivatives`,`they bracket the root automatically`],ans:0,exp:`Newton‑Raphson’s quadratic convergence beats the linear rate of bisection—when it converges.`},
{stem:`In floating‑point arithmetic, <em>chopping</em> digits (instead of rounding)`,opts:[`introduces a systematic negative bias`,`always provides a more accurate result`,`greatly reduces storage needs with no accuracy loss`,`is recommended for all numerical software`],ans:0,exp:`Chopping discards the trailing digits, so on average the stored value is slightly smaller than the exact number.`},
{stem:`The Taylor series expansion is used in numerical methods primarily to`,opts:[`approximate a function locally by a polynomial`,`eliminate all numerical error`,`convert a function into a system of linear equations`,`solve nonlinear equations exactly`],ans:0,exp:`Finite‑difference, interpolation, and many other schemes stem from truncating the Taylor series.`}
];
/* ===== Helper to create DOM elements ===== */
function html(strings,...values){return strings.reduce((result,str,i)=>result+str+(values[i]??""),"");}
const quizEl=document.getElementById("quiz");
quizData.forEach((q,idx)=>{
  const qDiv=document.createElement("div");
  qDiv.className="question";
  qDiv.innerHTML=html`<p><strong>Q${idx+1}. ${q.topic??""}</strong><br>${q.stem}</p>
    <div class="options">
      ${q.opts.map((opt,i)=>html`<label><input type="radio" name="q${idx}" value="${i}"> ${opt}</label>`).join("")}
    </div>
    <p class="feedback"></p>`;
  quizEl.appendChild(qDiv);
  const feedback=qDiv.querySelector(".feedback");
  qDiv.querySelectorAll("input[type=radio]").forEach(radio=>{
    radio.addEventListener("change",()=>{
      const selected=+radio.value;
      qDiv.querySelectorAll("input[type=radio]").forEach(r=>r.disabled=true);
      if(selected===q.ans){
        feedback.textContent="Correct! "+q.exp;
        feedback.className="feedback correct";
      }else{
        const correctText=q.opts[q.ans];
        feedback.innerHTML=`Incorrect. Correct answer: <em>${correctText}</em>. ${q.exp}`;
        feedback.className="feedback incorrect";
      }
    },{once:true});
  });
});
</script>
</body>
</html>
